{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavmeraga/pmeraga/blob/main/LSTM%2BCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, BatchNormalization, Dropout,\n",
        "                                   LSTM, Dense, TimeDistributed, Flatten, Input,\n",
        "                                   GlobalAveragePooling2D, Reshape, Concatenate)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import cv2\n",
        "import os\n",
        "import warnings\n",
        "import re\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class CNNLSTMKeyholeForecaster:\n",
        "    def __init__(self, sequence_length=20, forecast_horizon=5,\n",
        "                 img_height=128, img_width=128, channels=1,\n",
        "                 target_features=['melt_pool_width', 'melt_pool_depth', 'keyhole_width', 'keyhole_depth', 'porosity'],\n",
        "                 include_power_feature=True):\n",
        "        \"\"\"\n",
        "        Initialize the CNN-LSTM forecaster for keyhole dimensions from X-ray images\n",
        "\n",
        "        Parameters:\n",
        "        - sequence_length: Number of previous frames to use for prediction\n",
        "        - forecast_horizon: Number of future timesteps to predict\n",
        "        - img_height, img_width: Image dimensions (will be resized to this)\n",
        "        - channels: Number of image channels (1 for grayscale X-ray)\n",
        "        - target_features: Features to predict from images\n",
        "        - include_power_feature: Whether to include laser power as an additional input feature\n",
        "        \"\"\"\n",
        "        self.sequence_length = sequence_length\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.channels = channels\n",
        "        self.target_features = target_features\n",
        "        self.n_features = len(target_features)\n",
        "        self.include_power_feature = include_power_feature\n",
        "\n",
        "        # Scalers\n",
        "        self.target_scaler = MinMaxScaler()\n",
        "        self.power_scaler = MinMaxScaler()  # For laser power normalization\n",
        "        self.image_scaler = 1.0 / 255.0  # Normalize images to [0,1]\n",
        "\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.X_test = None\n",
        "        self.y_test = None\n",
        "        self.power_test = None\n",
        "\n",
        "    def extract_power_from_filename(self, filename):\n",
        "        \"\"\"\n",
        "        Extract laser power from filename\n",
        "\n",
        "        Expected format: 017_Al6061_P200W_t4ms_U18G16_spot0255_original.jpg\n",
        "        Returns: Power value (e.g., 200 for P200W)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Use regex to find P followed by digits and W\n",
        "            power_match = re.search(r'P(\\d+)W', filename)\n",
        "            if power_match:\n",
        "                return int(power_match.group(1))\n",
        "            else:\n",
        "                # Try alternative pattern if main one fails\n",
        "                alt_power_match = re.search(r'P(\\d+)_', filename)\n",
        "                if alt_power_match:\n",
        "                    return int(alt_power_match.group(1))\n",
        "                else:\n",
        "                    print(f\"Warning: Could not extract power from filename: {filename}\")\n",
        "                    return 200  # Default fallback\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting power from {filename}: {e}\")\n",
        "            return 200\n",
        "\n",
        "    def extract_spot_index_from_filename(self, filename):\n",
        "        \"\"\"\n",
        "        Extract spot index from filename\n",
        "\n",
        "        Expected format: 017_Al6061_P200W_t4ms_U18G16_spot0255_original.jpg\n",
        "        Returns: Spot index (e.g., 255 for spot0255)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Use regex to find spot followed by digits\n",
        "            spot_match = re.search(r'spot(\\d+)', filename)\n",
        "            if spot_match:\n",
        "                return int(spot_match.group(1))\n",
        "            else:\n",
        "                print(f\"Warning: Could not extract spot index from filename: {filename}\")\n",
        "                return 0  # Default fallback\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting spot index from {filename}: {e}\")\n",
        "            return 0\n",
        "\n",
        "    def load_trial_sequences(self, trial, trial_data, image_col='image_path'):\n",
        "        \"\"\"\n",
        "        Load all image sequences and labels for a single trial.\n",
        "\n",
        "        Parameters:\n",
        "        - trial: trial identifier\n",
        "        - trial_data: dataframe containing image paths and target labels for this trial\n",
        "        - image_col: column name containing image paths\n",
        "\n",
        "        Returns: X, y, power_sequences (sequences and future predictions)\n",
        "        \"\"\"\n",
        "        # Sort by spot index to ensure temporal order\n",
        "        if 'spot_index' in trial_data.columns:\n",
        "            trial_data = trial_data.sort_values('spot_index')\n",
        "        elif 'abs_index' in trial_data.columns:\n",
        "            trial_data = trial_data.sort_values('abs_index')\n",
        "        else:\n",
        "            trial_data = trial_data.sort_values(image_col)\n",
        "\n",
        "        images = self.load_and_preprocess_images(trial_data[image_col].values)\n",
        "\n",
        "        # Ensure target features are numeric before scaling\n",
        "        targets_df = trial_data[self.target_features].apply(pd.to_numeric, errors='coerce')\n",
        "        targets = self.target_scaler.transform(targets_df.values)\n",
        "\n",
        "        if self.include_power_feature:\n",
        "            # Ensure power values are numeric before scaling\n",
        "            power_values = pd.to_numeric(trial_data['laser_power'], errors='coerce').values\n",
        "            power_normalized = self.power_scaler.transform(power_values.reshape(-1, 1)).flatten()\n",
        "            return self.create_sequences_with_power(images, targets, power_normalized)\n",
        "        else:\n",
        "            return self.create_sequences(images, targets)\n",
        "\n",
        "    def load_and_preprocess_images(self, image_paths):\n",
        "        \"\"\"Load and preprocess images from file paths\"\"\"\n",
        "        images = []\n",
        "\n",
        "        for path in image_paths:\n",
        "            if not os.path.exists(path):\n",
        "                print(f\"Warning: Image not found: {path}\")\n",
        "                img = np.zeros((self.img_height, self.img_width), dtype=np.float32)\n",
        "            else:\n",
        "                img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "                if img is None:\n",
        "                    print(f\"Warning: Could not load image: {path}\")\n",
        "                    img = np.zeros((self.img_height, self.img_width), dtype=np.float32)\n",
        "                else:\n",
        "                    img = cv2.resize(img, (self.img_width, self.img_height))\n",
        "                    img = img.astype(np.float32) * self.image_scaler\n",
        "\n",
        "            img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
        "            images.append(img)\n",
        "\n",
        "        return np.array(images)\n",
        "\n",
        "    def create_sequences(self, images, targets):\n",
        "        \"\"\"\n",
        "        Create sequences for CNN-LSTM training (without power feature)\n",
        "\n",
        "        Parameters:\n",
        "        - images: Array of preprocessed images\n",
        "        - targets: Array of target values (keyhole dimensions)\n",
        "\n",
        "        Returns:\n",
        "        - X: Input image sequences\n",
        "        - y: Target sequences\n",
        "        \"\"\"\n",
        "        X, y = [], []\n",
        "\n",
        "        for i in range(len(images) - self.sequence_length - self.forecast_horizon + 1):\n",
        "            # Input sequence of images\n",
        "            X.append(images[i:(i + self.sequence_length)])\n",
        "\n",
        "            # Target sequence (multi-step ahead predictions)\n",
        "            y.append(targets[(i + self.sequence_length):(i + self.sequence_length + self.forecast_horizon)])\n",
        "\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    def create_sequences_with_power(self, images, targets, power_values):\n",
        "        \"\"\"\n",
        "        Create sequences for CNN-LSTM training with power feature\n",
        "\n",
        "        Parameters:\n",
        "        - images: Array of preprocessed images\n",
        "        - targets: Array of target values (keyhole dimensions)\n",
        "        - power_values: Array of normalized power values\n",
        "\n",
        "        Returns:\n",
        "        - X: Input image sequences\n",
        "        - y: Target sequences\n",
        "        - power_seq: Power value sequences\n",
        "        \"\"\"\n",
        "        X, y, power_seq = [], [], []\n",
        "\n",
        "        for i in range(len(images) - self.sequence_length - self.forecast_horizon + 1):\n",
        "            # Input sequence of images\n",
        "            X.append(images[i:(i + self.sequence_length)])\n",
        "\n",
        "            # Target sequence (multi-step ahead predictions)\n",
        "            y.append(targets[(i + self.sequence_length):(i + self.sequence_length + self.forecast_horizon)])\n",
        "\n",
        "            # Power sequence (current implementation uses the power at the start of sequence)\n",
        "            # You can modify this to use power at prediction time or average power\n",
        "            power_seq.append(power_values[i + self.sequence_length - 1])  # Power at end of input sequence\n",
        "\n",
        "        return np.array(X), np.array(y), np.array(power_seq)\n",
        "\n",
        "    def build_cnn_feature_extractor(self):\n",
        "        \"\"\"Build CNN feature extractor for individual frames\"\"\"\n",
        "        cnn_input = Input(shape=(self.img_height, self.img_width, self.channels))\n",
        "\n",
        "        x = Conv2D(32, 3, activation='relu', padding='same')(cnn_input)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D()(x)\n",
        "\n",
        "        x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D()(x)\n",
        "\n",
        "        x = Conv2D(128, 3, activation='relu', padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "        features = Dense(128, activation='relu', name='image_features')(x)\n",
        "        features = Dropout(0.3)(features)\n",
        "\n",
        "        return Model(inputs=cnn_input, outputs=features)\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Build the complete CNN-LSTM model with optional power input\"\"\"\n",
        "        # Image input\n",
        "        image_input = Input(shape=(self.sequence_length, self.img_height, self.img_width, self.channels),\n",
        "                           name='image_input')\n",
        "\n",
        "        # CNN feature extractor (applied to each frame)\n",
        "        cnn_model = self.build_cnn_feature_extractor()\n",
        "\n",
        "        # Apply CNN to each frame in the sequence\n",
        "        cnn_features = TimeDistributed(cnn_model)(image_input)\n",
        "\n",
        "        if self.include_power_feature:\n",
        "            # Power input (scalar value)\n",
        "            power_input = Input(shape=(1,), name='power_input')\n",
        "\n",
        "            # Repeat power value for each timestep in sequence\n",
        "            power_repeated = tf.keras.layers.RepeatVector(self.sequence_length)(power_input)\n",
        "\n",
        "            # Concatenate CNN features with power\n",
        "            combined_features = Concatenate(axis=-1)([cnn_features, power_repeated])\n",
        "\n",
        "            # LSTM layers for temporal modeling\n",
        "            lstm_out = LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(combined_features)\n",
        "            lstm_out = LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(lstm_out)\n",
        "            lstm_out = LSTM(32, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)(lstm_out)\n",
        "\n",
        "            # Include power in final layers\n",
        "            power_dense = Dense(16, activation='relu')(power_input)\n",
        "            combined_final = Concatenate()([lstm_out, power_dense])\n",
        "\n",
        "            # Dense layers for prediction\n",
        "            x = Dense(64, activation='relu')(combined_final)\n",
        "            x = Dropout(0.3)(x)\n",
        "            x = Dense(32, activation='relu')(x)\n",
        "            x = Dropout(0.2)(x)\n",
        "\n",
        "            # Output layer\n",
        "            output = Dense(self.forecast_horizon * self.n_features, activation='linear')(x)\n",
        "            output = Reshape((self.forecast_horizon, self.n_features))(output)\n",
        "\n",
        "            model = Model(inputs=[image_input, power_input], outputs=output)\n",
        "\n",
        "        else:\n",
        "            # LSTM layers for temporal modeling (without power)\n",
        "            lstm_out = LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(cnn_features)\n",
        "            lstm_out = LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(lstm_out)\n",
        "            lstm_out = LSTM(32, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)(lstm_out)\n",
        "\n",
        "            # Dense layers for prediction\n",
        "            x = Dense(64, activation='relu')(lstm_out)\n",
        "            x = Dropout(0.3)(x)\n",
        "            x = Dense(32, activation='relu')(x)\n",
        "            x = Dropout(0.2)(x)\n",
        "\n",
        "            # Output layer\n",
        "            output = Dense(self.forecast_horizon * self.n_features, activation='linear')(x)\n",
        "            output = Reshape((self.forecast_horizon, self.n_features))(output)\n",
        "\n",
        "            model = Model(inputs=image_input, outputs=output)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def prepare_data_by_trial(self, df_combined, train_trials, val_trials, test_trials):\n",
        "        \"\"\"\n",
        "        Prepare training/validation/test data by trial splitting\n",
        "\n",
        "        Parameters:\n",
        "        - df_combined: Combined dataframe with all trials\n",
        "        - train_trials, val_trials, test_trials: Lists of trial names for each split\n",
        "\n",
        "        Returns:\n",
        "        - Tuple of (train_data, val_data, test_data)\n",
        "        \"\"\"\n",
        "        X_train, y_train, power_train = [], [], []\n",
        "        X_val, y_val, power_val = [], [], []\n",
        "        X_test, y_test, power_test = [], [], []\n",
        "\n",
        "        for trial in df_combined['trial'].unique():\n",
        "            trial_data = df_combined[df_combined['trial'] == trial]\n",
        "\n",
        "            print(f\"Processing trial {trial}: {len(trial_data)} samples\")\n",
        "\n",
        "            # Load trial sequences\n",
        "            if self.include_power_feature:\n",
        "                trial_X, trial_y, trial_power = self.load_trial_sequences(trial, trial_data, image_col='image_path')\n",
        "            else:\n",
        "                trial_X, trial_y = self.load_trial_sequences(trial, trial_data, image_col='image_path')\n",
        "                trial_power = None\n",
        "\n",
        "            if len(trial_X) == 0:\n",
        "                print(f\"Warning: No sequences generated for trial {trial}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"  Generated {len(trial_X)} sequences\")\n",
        "\n",
        "            # Assign to appropriate split\n",
        "            if trial in train_trials:\n",
        "                X_train.append(trial_X)\n",
        "                y_train.append(trial_y)\n",
        "                if self.include_power_feature:\n",
        "                    power_train.append(trial_power)\n",
        "            elif trial in val_trials:\n",
        "                X_val.append(trial_X)\n",
        "                y_val.append(trial_y)\n",
        "                if self.include_power_feature:\n",
        "                    power_val.append(trial_power)\n",
        "            elif trial in test_trials:\n",
        "                X_test.append(trial_X)\n",
        "                y_test.append(trial_y)\n",
        "                if self.include_power_feature:\n",
        "                    power_test.append(trial_power)\n",
        "\n",
        "        # Concatenate arrays\n",
        "        X_train = np.concatenate(X_train) if X_train else np.array([])\n",
        "        y_train = np.concatenate(y_train) if y_train else np.array([])\n",
        "        X_val = np.concatenate(X_val) if X_val else np.array([])\n",
        "        y_val = np.concatenate(y_val) if y_val else np.array([])\n",
        "        X_test = np.concatenate(X_test) if X_test else np.array([])\n",
        "        y_test = np.concatenate(y_test) if y_test else np.array([])\n",
        "\n",
        "        if self.include_power_feature:\n",
        "            power_train = np.concatenate(power_train) if power_train else np.array([])\n",
        "            power_val = np.concatenate(power_val) if power_val else np.array([])\n",
        "            power_test = np.concatenate(power_test) if power_test else np.array([])\n",
        "        else:\n",
        "            power_train = power_val = power_test = None\n",
        "\n",
        "        print(f\"\\nFinal data shapes:\")\n",
        "        print(f\"  Train: X={X_train.shape}, y={y_train.shape}\", end=\"\")\n",
        "        if self.include_power_feature:\n",
        "            print(f\", power={power_train.shape}\")\n",
        "        else:\n",
        "            print()\n",
        "        print(f\"  Val:   X={X_val.shape}, y={y_val.shape}\", end=\"\")\n",
        "        if self.include_power_feature:\n",
        "            print(f\", power={power_val.shape}\")\n",
        "        else:\n",
        "            print()\n",
        "        print(f\"  Test:  X={X_test.shape}, y={y_test.shape}\", end=\"\")\n",
        "        if self.include_power_feature:\n",
        "            print(f\", power={power_test.shape}\")\n",
        "        else:\n",
        "            print()\n",
        "\n",
        "        return (X_train, y_train, power_train), (X_val, y_val, power_val), (X_test, y_test, power_test)\n",
        "\n",
        "    def train(self, df_combined, epochs=100, batch_size=16,\n",
        "              split_strategy='trial', test_trials=[], val_trials=[],\n",
        "              verbose=1):\n",
        "        \"\"\"\n",
        "        Train the CNN-LSTM model\n",
        "\n",
        "        Parameters:\n",
        "        - df_combined: Combined dataframe with all trial data\n",
        "        - epochs: Number of training epochs\n",
        "        - batch_size: Training batch size\n",
        "        - split_strategy: Only 'trial' supported in this integrated version\n",
        "        - test_trials, val_trials: Lists of trial names for splits\n",
        "        - verbose: Training verbosity\n",
        "        \"\"\"\n",
        "        if split_strategy != 'trial':\n",
        "            raise ValueError(\"Only 'trial' split strategy is supported in this integrated version\")\n",
        "\n",
        "        # Prepare data with trial-based splitting\n",
        "        (X_train, y_train, power_train), (X_val, y_val, power_val), (X_test, y_test, power_test) = self.prepare_data_by_trial(\n",
        "            df_combined,\n",
        "            [t for t in df_combined['trial'].unique() if t not in test_trials + val_trials],\n",
        "            val_trials,\n",
        "            test_trials\n",
        "        )\n",
        "\n",
        "        if len(X_train) == 0:\n",
        "            raise ValueError(\"No training data generated. Check your data and parameters.\")\n",
        "\n",
        "        # Store test data for evaluation\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        self.power_test = power_test\n",
        "\n",
        "        # Build model\n",
        "        print(\"Building CNN-LSTM model...\")\n",
        "        self.model = self.build_model()\n",
        "        print(\"Model architecture:\")\n",
        "        self.model.summary()\n",
        "\n",
        "        # Define callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=20,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=15,\n",
        "                min_lr=1e-6,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                'best_cnn_lstm_model.h5',\n",
        "                monitor='val_loss',\n",
        "                save_best_only=True,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Prepare training inputs\n",
        "        if self.include_power_feature:\n",
        "            train_inputs = [X_train, power_train]\n",
        "            val_inputs = [X_val, power_val] if len(X_val) > 0 else None\n",
        "        else:\n",
        "            train_inputs = X_train\n",
        "            val_inputs = X_val if len(X_val) > 0 else None\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Training model...\")\n",
        "        validation_data = (val_inputs, y_val) if val_inputs is not None and len(X_val) > 0 else None\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            train_inputs, y_train,\n",
        "            validation_data=validation_data,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        \"\"\"Evaluate model performance on test set\"\"\"\n",
        "        if self.X_test is None or self.y_test is None or len(self.X_test) == 0:\n",
        "            raise ValueError(\"No test data available. Train the model first.\")\n",
        "\n",
        "        # Make predictions\n",
        "        if self.include_power_feature:\n",
        "            test_inputs = [self.X_test, self.power_test]\n",
        "        else:\n",
        "            test_inputs = self.X_test\n",
        "\n",
        "        y_pred = self.model.predict(test_inputs, verbose=0)\n",
        "\n",
        "        # Calculate metrics for each feature\n",
        "        results = {}\n",
        "        for i, feature in enumerate(self.target_features):\n",
        "            y_true_flat = self.y_test[:, :, i].flatten()\n",
        "            y_pred_flat = y_pred[:, :, i].flatten()\n",
        "\n",
        "            results[feature] = {\n",
        "                'mse': mean_squared_error(y_true_flat, y_pred_flat),\n",
        "                'rmse': np.sqrt(mean_squared_error(y_true_flat, y_pred_flat)),\n",
        "                'mae': mean_absolute_error(y_true_flat, y_pred_flat),\n",
        "                'r2': r2_score(y_true_flat, y_pred_flat)\n",
        "            }\n",
        "\n",
        "        return results, y_pred\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Plot training history\"\"\"\n",
        "        if self.history is None:\n",
        "            raise ValueError(\"No training history available.\")\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Plot loss\n",
        "        axes[0].plot(self.history.history['loss'], label='Training Loss')\n",
        "        if 'val_loss' in self.history.history:\n",
        "            axes[0].plot(self.history.history['val_loss'], label='Validation Loss')\n",
        "        axes[0].set_title('Model Loss')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Loss')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True)\n",
        "\n",
        "        # Plot MAE\n",
        "        axes[1].plot(self.history.history['mae'], label='Training MAE')\n",
        "        if 'val_mae' in self.history.history:\n",
        "            axes[1].plot(self.history.history['val_mae'], label='Validation MAE')\n",
        "        axes[1].set_title('Model MAE')\n",
        "        axes[1].set_xlabel('Epoch')\n",
        "        axes[1].set_ylabel('MAE')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_predictions(self, n_samples=3):\n",
        "        \"\"\"Plot sample predictions vs actual values\"\"\"\n",
        "        if self.X_test is None or len(self.X_test) == 0:\n",
        "            raise ValueError(\"No test data available.\")\n",
        "\n",
        "        n_samples = min(n_samples, len(self.X_test))\n",
        "\n",
        "        if self.include_power_feature:\n",
        "            test_inputs = [self.X_test[:n_samples], self.power_test[:n_samples]]\n",
        "        else:\n",
        "            test_inputs = self.X_test[:n_samples]\n",
        "\n",
        "        y_pred = self.model.predict(test_inputs, verbose=0)\n",
        "\n",
        "        # Inverse transform to original scale\n",
        "        y_true_reshaped = self.y_test[:n_samples].reshape(-1, self.n_features)\n",
        "        y_pred_reshaped = y_pred.reshape(-1, self.n_features)\n",
        "\n",
        "        y_true_orig = self.target_scaler.inverse_transform(y_true_reshaped)\n",
        "        y_pred_orig = self.target_scaler.inverse_transform(y_pred_reshaped)\n",
        "\n",
        "        # Reshape back\n",
        "        y_true_orig = y_true_orig.reshape(n_samples, self.forecast_horizon, self.n_features)\n",
        "        y_pred_orig = y_pred_orig.reshape(n_samples, self.forecast_horizon, self.n_features)\n",
        "\n",
        "        # Plot for each feature\n",
        "        fig, axes = plt.subplots(len(self.target_features), 1,\n",
        "                               figsize=(15, 3 * len(self.target_features)))\n",
        "        if len(self.target_features) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i, feature in enumerate(self.target_features):\n",
        "            for j in range(n_samples):\n",
        "                label_suffix = \"\"\n",
        "                if self.include_power_feature:\n",
        "                    power_orig = self.power_scaler.inverse_transform([[self.power_test[j]]])[0][0]\n",
        "                    label_suffix = f\" (P={power_orig:.0f}W)\"\n",
        "\n",
        "                axes[i].plot(range(self.forecast_horizon), y_true_orig[j, :, i],\n",
        "                           'o-', label=f'Actual (Sample {j+1}{label_suffix})', alpha=0.7)\n",
        "                axes[i].plot(range(self.forecast_horizon), y_pred_orig[j, :, i],\n",
        "                           's--', label=f'Predicted (Sample {j+1}{label_suffix})', alpha=0.7)\n",
        "\n",
        "            axes[i].set_title(f'{feature} - Predictions vs Actual')\n",
        "            axes[i].set_xlabel('Forecast Step')\n",
        "            axes[i].set_ylabel('Value')\n",
        "            axes[i].legend()\n",
        "            axes[i].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def load_data_from_folders(image_base_path, label_base_path):\n",
        "    \"\"\"\n",
        "    Load data where each CSV corresponds to a folder of images\n",
        "\n",
        "    Expected structure:\n",
        "    - images/\n",
        "      - 017_Al6061_P200W_t4ms_U18G16/\n",
        "        - 017_Al6061_P200W_t4ms_U18G16_spot0255_original.jpg\n",
        "        - 017_Al6061_P200W_t4ms_U18G16_spot0256_original.jpg\n",
        "        - ...\n",
        "      - 017_Al6061_P250W_t4ms_U18G16/\n",
        "        - 017_Al6061_P250W_t4ms_U18G16_spot0255_original.jpg\n",
        "        - ...\n",
        "    - labels/\n",
        "      - AM_label_017_Al6061_P200W_t4ms_U18G16.csv\n",
        "      - AM_label_017_Al6061_P250W_t4ms_U18G16.csv\n",
        "      - ...\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "\n",
        "    # Get all CSV files\n",
        "    if not os.path.exists(label_base_path):\n",
        "        raise ValueError(f\"Label directory not found: {label_base_path}\")\n",
        "\n",
        "    csv_files = [f for f in os.listdir(label_base_path) if f.endswith('.csv')]\n",
        "    print(f\"Found {len(csv_files)} CSV files: {csv_files}\")\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        # Extract trial name from CSV filename\n",
        "        # Remove 'AM_label_' prefix and '.csv' suffix\n",
        "        trial_name = csv_file.replace('.csv', '').replace('AM_label_', '')\n",
        "\n",
        "        # Load CSV\n",
        "        csv_path = os.path.join(label_base_path, csv_file)\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"\\nLoading {csv_file}: {df.shape}\")\n",
        "        print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "        # Create image folder path\n",
        "        image_folder = os.path.join(image_base_path, trial_name)\n",
        "\n",
        "        if os.path.exists(image_folder):\n",
        "            # Get available image files\n",
        "            image_files = sorted([f for f in os.listdir(image_folder)\n",
        "                                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff'))])\n",
        "            print(f\"Found {len(image_files)} images in {trial_name}\")\n",
        "\n",
        "            # Parse first image to understand the naming pattern\n",
        "            image_path_template = None\n",
        "            if image_files:\n",
        "                sample_image_filename = image_files[0]\n",
        "                print(f\"Sample image filename: {sample_image_filename}\")\n",
        "                # Try to extract the full filename structure before the spot index\n",
        "                match = re.match(r'(.*_spot)\\d+_original\\.jpg', sample_image_filename)\n",
        "                if match:\n",
        "                    prefix = match.group(1)\n",
        "                    image_path_template = os.path.join(image_folder, f\"{prefix}{{x:04d}}_original.jpg\")\n",
        "                    print(f\"Inferred image path template: {image_path_template}\")\n",
        "                else:\n",
        "                    print(f\"Could not infer image path template from {sample_image_filename}\")\n",
        "                    # Fallback to the previous method if template inference fails\n",
        "                    if 'spot_index' in df.columns:\n",
        "                        image_path_template = os.path.join(image_folder, f\"{trial_name}_spot{{x:04d}}_original.jpg\")\n",
        "                        print(f\"Using fallback image path template based on spot_index: {image_path_template}\")\n",
        "                    elif 'abs_index' in df.columns:\n",
        "                        image_path_template = os.path.join(image_folder, f\"{trial_name}_spot{{x:04d}}_original.jpg\")\n",
        "                        print(f\"Using fallback image path template based on abs_index: {image_path_template}\")\n",
        "\n",
        "\n",
        "            if image_path_template and ('spot_index' in df.columns or 'abs_index' in df.columns):\n",
        "                # Use the inferred or fallback template with available index\n",
        "                index_col = 'spot_index' if 'spot_index' in df.columns else 'abs_index'\n",
        "                df['image_path'] = df[index_col].apply(lambda x: image_path_template.format(x=x))\n",
        "                print(f\"Generated image paths using column '{index_col}' and template.\")\n",
        "\n",
        "                # Verify if generated paths exist for the first few rows\n",
        "                existing_paths_check = df['image_path'].head().apply(os.path.exists)\n",
        "                if not existing_paths_check.all():\n",
        "                    print(f\"Warning: Generated image paths using template do not exist for {trial_name}\")\n",
        "                    for path in df['image_path'].head()[~existing_paths_check]:\n",
        "                         print(f\"  Generated but missing: {path}\")\n",
        "                    # If generated paths don't exist, try sequential matching as a last resort\n",
        "                    if len(image_files) == len(df):\n",
        "                        df['image_path'] = [os.path.join(image_folder, f) for f in image_files]\n",
        "                        print(\"Falling back to sequential image matching\")\n",
        "                    else:\n",
        "                        print(f\"Warning: Sequential matching not possible (Mismatch between CSV rows ({len(df)}) and images ({len(image_files)}) in {trial_name})\")\n",
        "                        continue # Skip trial if no reliable method works\n",
        "\n",
        "            # Method 3: Sequential matching if no index columns or template failed\n",
        "            elif len(image_files) == len(df):\n",
        "                df['image_path'] = [os.path.join(image_folder, f) for f in image_files]\n",
        "                print(\"Using sequential image matching\")\n",
        "            else:\n",
        "                print(f\"Warning: Mismatch between CSV rows ({len(df)}) and images ({len(image_files)}) and no index/template method worked for {trial_name}\")\n",
        "                continue # Skip trial if no reliable method works\n",
        "\n",
        "\n",
        "            # Extract additional features from image filenames (using a sample filename)\n",
        "            if image_files: # Ensure there's at least one image to sample from\n",
        "                 forecaster_temp = CNNLSTMKeyholeForecaster() # Re-initialize for safe use of methods\n",
        "                 df['laser_power'] = df['image_path'].apply(\n",
        "                     lambda x: forecaster_temp.extract_power_from_filename(os.path.basename(x))\n",
        "                 )\n",
        "                 df['spot_index'] = df['image_path'].apply(\n",
        "                     lambda x: forecaster_temp.extract_spot_index_from_filename(os.path.basename(x))\n",
        "                 )\n",
        "            else:\n",
        "                 print(f\"Warning: No images found in {image_folder}, cannot extract features.\")\n",
        "                 continue # Skip trial if no images\n",
        "\n",
        "\n",
        "            # Verify some paths exist\n",
        "            existing_paths = df['image_path'].head(5).apply(os.path.exists)\n",
        "            if not existing_paths.all():\n",
        "                print(f\"Warning: Some image paths don't exist in {trial_name} even after generation attempts.\")\n",
        "                missing_paths = df['image_path'].head(5)[~existing_paths]\n",
        "                for path in missing_paths:\n",
        "                    print(f\"  Missing: {path}\")\n",
        "                continue # Skip trial if image paths are still missing\n",
        "\n",
        "            else:\n",
        "                print(f\"✓ Verified image paths exist\")\n",
        "\n",
        "            # Print power distribution for this trial\n",
        "            unique_powers = df['laser_power'].unique()\n",
        "            print(f\"Laser powers in this trial: {unique_powers}\")\n",
        "\n",
        "            # Add trial identifier\n",
        "            df['trial'] = trial_name\n",
        "            all_data.append(df)\n",
        "            print(f\"✓ Loaded {len(df)} samples from {trial_name}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"✗ Warning: Image folder {image_folder} not found for {csv_file}\")\n",
        "\n",
        "    if not all_data:\n",
        "        raise ValueError(\"No data loaded! Check your folder structure and paths.\")\n",
        "\n",
        "    # Combine all data\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "    # Convert target features and laser power to numeric, coercing errors\n",
        "    forecaster_temp = CNNLSTMKeyholeForecaster() # Re-initialize for safe use\n",
        "    features_to_convert = list(forecaster_temp.target_features) + ['laser_power']\n",
        "    for feature in features_to_convert:\n",
        "        if feature in combined_df.columns:\n",
        "             combined_df[feature] = pd.to_numeric(combined_df[feature], errors='coerce')\n",
        "\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*50)\n",
        "    print(f\"COMBINED DATASET\")\n",
        "    print(f\"=\"*50)\n",
        "    print(f\"Total samples: {len(combined_df)}\")\n",
        "    print(f\"Trials: {sorted(combined_df['trial'].unique())}\")\n",
        "    print(f\"Laser power range: {combined_df['laser_power'].min()}W - {combined_df['laser_power'].max()}W\")\n",
        "    print(f\"Unique laser powers: {sorted(combined_df['laser_power'].unique())}\")\n",
        "    if not combined_df.empty:\n",
        "        print(f\"Sample image path: {combined_df['image_path'].iloc[0]}\")\n",
        "        print(f\"Image exists: {os.path.exists(combined_df['image_path'].iloc[0])}\")\n",
        "    else:\n",
        "         print(\"Combined dataframe is empty.\")\n",
        "\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "\n",
        "def define_trial_splits(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_seed=42):\n",
        "    \"\"\"Split trials into train/validation/test sets ensuring power distribution\"\"\"\n",
        "    trials = sorted(df['trial'].unique())\n",
        "    n_trials = len(trials)\n",
        "\n",
        "    if n_trials == 0:\n",
        "        print(\"Warning: No trials available for splitting.\")\n",
        "        return [], [], []\n",
        "\n",
        "    # Get power information for each trial\n",
        "    trial_powers = {}\n",
        "    for trial in trials:\n",
        "        trial_data = df[df['trial'] == trial]\n",
        "        if not trial_data.empty and 'laser_power' in trial_data.columns:\n",
        "             trial_powers[trial] = trial_data['laser_power'].iloc[0]  # Assuming consistent power per trial\n",
        "        else:\n",
        "             print(f\"Warning: Cannot get power for trial {trial}, skipping.\")\n",
        "             continue # Skip trials with no data or missing power\n",
        "\n",
        "    # Filter out trials that couldn't get power information\n",
        "    valid_trials = list(trial_powers.keys())\n",
        "    n_valid_trials = len(valid_trials)\n",
        "\n",
        "    if n_valid_trials < 3: # Need at least 3 trials for train/val/test split\n",
        "         print(f\"Warning: Only {n_valid_trials} valid trials found. Cannot perform train/val/test split.\")\n",
        "         return valid_trials, [], [] # Return all valid trials as training\n",
        "\n",
        "    print(f\"\\nTrial power distribution (from {n_valid_trials} valid trials):\")\n",
        "    for trial, power in trial_powers.items():\n",
        "        print(f\"  {trial}: {power}W\")\n",
        "\n",
        "\n",
        "    # Calculate split indices (adjust based on valid trials)\n",
        "    n_train = int(n_valid_trials * train_ratio)\n",
        "    n_val = int(n_valid_trials * val_ratio)\n",
        "\n",
        "    # Ensure at least one trial in train and test if possible\n",
        "    if n_train == 0 and n_valid_trials > 0: n_train = 1\n",
        "    if n_valid_trials - n_train - n_val == 0 and n_valid_trials > n_train: n_val = 1 # Ensure at least one val if possible\n",
        "\n",
        "    # Stratified split to ensure power distribution is maintained across splits\n",
        "    import random\n",
        "    random.seed(random_seed)\n",
        "\n",
        "    # Group trials by power (using only valid trials)\n",
        "    power_groups = {}\n",
        "    for trial in valid_trials:\n",
        "        power = trial_powers[trial]\n",
        "        if power not in power_groups:\n",
        "            power_groups[power] = []\n",
        "        power_groups[power].append(trial)\n",
        "\n",
        "    # Shuffle trials within each power group\n",
        "    for power in power_groups:\n",
        "        random.shuffle(power_groups[power])\n",
        "\n",
        "    # Distribute trials from each power group across splits\n",
        "    train_trials, val_trials, test_trials = [], [], []\n",
        "    all_valid_trials_shuffled = []\n",
        "    for power in sorted(power_groups.keys()):\n",
        "         all_valid_trials_shuffled.extend(power_groups[power])\n",
        "\n",
        "\n",
        "    # Simple sequential split from shuffled list if stratified is complex with small numbers\n",
        "    train_trials = all_valid_trials_shuffled[:n_train]\n",
        "    val_trials = all_valid_trials_shuffled[n_train:n_train + n_val]\n",
        "    test_trials = all_valid_trials_shuffled[n_train + n_val:]\n",
        "\n",
        "\n",
        "    print(f\"\\nStratified trial splits (seed={random_seed}):\")\n",
        "    print(f\"  Train trials ({len(train_trials)}): {train_trials}\")\n",
        "    print(f\"  Val trials ({len(val_trials)}): {val_trials}\")\n",
        "    print(f\"  Test trials ({len(test_trials)}): {test_trials}\")\n",
        "\n",
        "    # Show data and power distribution\n",
        "    for split_name, trial_list in [('Train', train_trials), ('Val', val_trials), ('Test', test_trials)]:\n",
        "        split_data = df[df['trial'].isin(trial_list)]\n",
        "        if not split_data.empty:\n",
        "            powers_in_split = sorted(split_data['laser_power'].unique())\n",
        "            print(f\"  {split_name}: {len(split_data)} samples, Powers: {powers_in_split}W\")\n",
        "        else:\n",
        "            print(f\"  {split_name}: 0 samples\")\n",
        "\n",
        "\n",
        "    return train_trials, val_trials, test_trials\n",
        "\n",
        "\n",
        "def analyze_power_vs_performance(forecaster, df_combined):\n",
        "    \"\"\"\n",
        "    Analyze how model performance varies with laser power\n",
        "    \"\"\"\n",
        "    if forecaster.X_test is None or not forecaster.include_power_feature or len(forecaster.X_test) == 0:\n",
        "        print(\"No test data or power feature not included, or test data is empty.\")\n",
        "        return\n",
        "\n",
        "    # Make predictions\n",
        "    test_inputs = [forecaster.X_test, forecaster.power_test]\n",
        "    y_pred = forecaster.model.predict(test_inputs, verbose=0)\n",
        "\n",
        "    # Get original power values\n",
        "    # Ensure power_test is not empty\n",
        "    if forecaster.power_test is not None and len(forecaster.power_test) > 0:\n",
        "        power_orig = forecaster.power_scaler.inverse_transform(forecaster.power_test.reshape(-1, 1)).flatten()\n",
        "    else:\n",
        "        print(\"No power data available for power vs performance analysis.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    # Calculate metrics by power level\n",
        "    unique_powers = np.unique(power_orig)\n",
        "    power_metrics = {}\n",
        "\n",
        "    for power in unique_powers:\n",
        "        power_mask = power_orig == power\n",
        "        if np.sum(power_mask) == 0:\n",
        "            continue\n",
        "\n",
        "        power_metrics[power] = {}\n",
        "\n",
        "        for i, feature in enumerate(forecaster.target_features):\n",
        "            y_true_power = forecaster.y_test[power_mask, :, i].flatten()\n",
        "            y_pred_power = y_pred[power_mask, :, i].flatten()\n",
        "\n",
        "            # Ensure there are samples for this power level and feature\n",
        "            if len(y_true_power) > 0:\n",
        "                power_metrics[power][feature] = {\n",
        "                    'rmse': np.sqrt(mean_squared_error(y_true_power, y_pred_power)),\n",
        "                    'mae': mean_absolute_error(y_true_power, y_pred_power),\n",
        "                    'r2': r2_score(y_true_power, y_pred_power),\n",
        "                    'n_samples': len(y_true_power)\n",
        "                }\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nPerformance by Laser Power:\")\n",
        "    print(\"=\" * 60)\n",
        "    if not power_metrics:\n",
        "        print(\"No performance data available by power level.\")\n",
        "        return power_metrics\n",
        "\n",
        "    for power in sorted(power_metrics.keys()):\n",
        "        print(f\"\\nPower: {power:.0f}W\")\n",
        "        for feature in forecaster.target_features:\n",
        "            if feature in power_metrics[power]:\n",
        "                metrics = power_metrics[power][feature]\n",
        "                print(f\"  {feature}:\")\n",
        "                print(f\"    RMSE: {metrics['rmse']:.4f}\")\n",
        "                print(f\"    MAE:  {metrics['mae']:.4f}\")\n",
        "                print(f\"    R²:   {metrics['r2']:.4f}\")\n",
        "                print(f\"    Samples: {metrics['n_samples']}\")\n",
        "            else:\n",
        "                 print(f\"  {feature}: No data for this power level.\")\n",
        "\n",
        "\n",
        "    return power_metrics\n",
        "\n",
        "\n",
        "def plot_power_analysis(forecaster, df_combined):\n",
        "    \"\"\"\n",
        "    Create visualizations of power vs model performance\n",
        "    \"\"\"\n",
        "    if not forecaster.include_power_feature:\n",
        "        print(\"Power feature not included in model\")\n",
        "        return\n",
        "\n",
        "    power_metrics = analyze_power_vs_performance(forecaster, df_combined)\n",
        "    if not power_metrics:\n",
        "        print(\"No power metrics available to plot.\")\n",
        "        return\n",
        "\n",
        "    powers = sorted(power_metrics.keys())\n",
        "    n_features = len(forecaster.target_features)\n",
        "\n",
        "    # Determine how many subplots are needed for metrics + distribution\n",
        "    num_metric_plots = min(len(['rmse', 'mae', 'r2']), n_features)\n",
        "    num_plots = num_metric_plots + 1 # metrics + distribution\n",
        "    n_cols = 3 # Max columns\n",
        "    n_rows = (num_plots + n_cols - 1) // n_cols # Calculate rows needed\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    metrics_to_plot = ['rmse', 'mae', 'r2']\n",
        "    current_plot_idx = 0\n",
        "\n",
        "    for i, metric in enumerate(metrics_to_plot):\n",
        "        if current_plot_idx >= len(axes) -1: # Leave space for distribution plot\n",
        "             break\n",
        "\n",
        "        # Check if metric data is available for at least one power/feature combination\n",
        "        has_metric_data = False\n",
        "        for power in powers:\n",
        "            for feature in forecaster.target_features:\n",
        "                if feature in power_metrics[power] and metric in power_metrics[power][feature]:\n",
        "                    has_metric_data = True\n",
        "                    break\n",
        "            if has_metric_data: break\n",
        "\n",
        "        if has_metric_data:\n",
        "            ax = axes[current_plot_idx]\n",
        "            current_plot_idx += 1\n",
        "\n",
        "            for j, feature in enumerate(forecaster.target_features):\n",
        "                values = [power_metrics[power].get(feature, {}).get(metric, None) for power in powers]\n",
        "                # Filter out None values for plotting\n",
        "                plottable_values = [v for v in values if v is not None]\n",
        "                plottable_powers = [p for p, v in zip(powers, values) if v is not None]\n",
        "\n",
        "                if plottable_values:\n",
        "                    ax.plot(plottable_powers, plottable_values, 'o-', label=feature, alpha=0.7)\n",
        "\n",
        "\n",
        "            ax.set_xlabel('Laser Power (W)')\n",
        "            ax.set_ylabel(metric.upper())\n",
        "            ax.set_title(f'{metric.upper()} vs Laser Power')\n",
        "            ax.legend()\n",
        "            ax.grid(True)\n",
        "        else:\n",
        "            print(f\"No data available to plot {metric.upper()}.\")\n",
        "\n",
        "\n",
        "    # Power distribution plot (always last)\n",
        "    if current_plot_idx < len(axes):\n",
        "        ax = axes[current_plot_idx]\n",
        "        current_plot_idx += 1\n",
        "\n",
        "        if not df_combined.empty and 'laser_power' in df_combined.columns:\n",
        "            power_counts = df_combined['laser_power'].value_counts().sort_index()\n",
        "            if not power_counts.empty:\n",
        "                ax.bar(power_counts.index, power_counts.values, alpha=0.7)\n",
        "                ax.set_xlabel('Laser Power (W)')\n",
        "                ax.set_ylabel('Number of Samples')\n",
        "                ax.set_title('Data Distribution by Laser Power')\n",
        "                ax.set_xticks(power_counts.index) # Ensure all power levels are shown\n",
        "                ax.grid(True)\n",
        "            else:\n",
        "                 ax.set_title('No data available for Power Distribution')\n",
        "        else:\n",
        "            ax.set_title('No data available for Power Distribution')\n",
        "\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for i in range(current_plot_idx, len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# MAIN EXECUTION SCRIPT\n",
        "# ====================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"CNN-LSTM KEYHOLE FORECASTER WITH LASER POWER\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. Mount Google Drive\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # 2. CONFIGURE YOUR PATHS\n",
        "    # UPDATE THESE PATHS TO MATCH YOUR GOOGLE DRIVE STRUCTURE\n",
        "    BASE_PATH = '/content/drive/MyDrive/label_keyhole_porosity'\n",
        "    IMAGE_BASE_PATH = f'{BASE_PATH}/AM_image'  # Folder containing image subfolders\n",
        "    LABEL_BASE_PATH = f'{BASE_PATH}/AM_label'  # Folder containing CSV files\n",
        "\n",
        "    print(f\"Image base path: {IMAGE_BASE_PATH}\")\n",
        "    print(f\"Label base path: {LABEL_BASE_PATH}\")\n",
        "\n",
        "    # 3. Load and combine all data\n",
        "    try:\n",
        "        print(f\"\\nLoading data from folders...\")\n",
        "        df_combined = load_data_from_folders(IMAGE_BASE_PATH, LABEL_BASE_PATH)\n",
        "        if df_combined.empty:\n",
        "            print(\"ERROR: Data loading resulted in an empty dataset. Exiting.\")\n",
        "            exit(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading data: {e}\")\n",
        "        print(\"\\nPlease check:\")\n",
        "        print(\"1. Your Google Drive paths are correct\")\n",
        "        print(\"2. The folder structure matches the expected format\")\n",
        "        print(\"3. CSV files and image folders have matching names\")\n",
        "        print(\"4. Image files follow the naming convention: XXX_Al6061_PXXXW_t4ms_U18G16_spotXXXX_original.jpg\")\n",
        "        exit(1)\n",
        "\n",
        "    # 4. Configure target features\n",
        "    print(f\"\\nAvailable columns: {list(df_combined.columns)}\")\n",
        "\n",
        "    # UPDATE THESE TO MATCH YOUR ACTUAL CSV COLUMNS\n",
        "    target_features = [\n",
        "        'v2_keyhole_depth/pixel',\n",
        "        'v2_keyhole_width/pixel',\n",
        "        'v2_keyhole_depth/um',\n",
        "        'v2_keyhole_width/um',\n",
        "        'porosity'\n",
        "    ]\n",
        "\n",
        "    # Verify target features exist\n",
        "    available_features = [f for f in target_features if f in df_combined.columns]\n",
        "    missing_features = [f for f in target_features if f not in df_combined.columns]\n",
        "\n",
        "    if missing_features:\n",
        "        print(f\"Warning: Missing features: {missing_features}\")\n",
        "        print(\"Available numeric columns:\")\n",
        "        numeric_cols = df_combined.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        print(numeric_cols)\n",
        "        target_features = available_features\n",
        "\n",
        "    if not target_features:\n",
        "        print(\"ERROR: No valid target features found!\")\n",
        "        exit(1)\n",
        "\n",
        "    print(f\"Using target features: {target_features}\")\n",
        "    print(f\"\\nTarget feature statistics:\")\n",
        "    print(df_combined[target_features].describe())\n",
        "\n",
        "    # 5. Split trials with power stratification\n",
        "    train_trials, val_trials, test_trials = define_trial_splits(\n",
        "        df_combined,\n",
        "        train_ratio=0.7,\n",
        "        val_ratio=0.15,\n",
        "        test_ratio=0.15,\n",
        "        random_seed=42\n",
        "    )\n",
        "\n",
        "    if len(train_trials) == 0:\n",
        "        print(\"ERROR: No training trials assigned after splitting. Cannot proceed with training. Check your data and split ratios.\")\n",
        "        exit(1)\n",
        "\n",
        "    # 6. Initialize forecaster with power feature\n",
        "    forecaster = CNNLSTMKeyholeForecaster(\n",
        "        sequence_length=20,      # Number of frames to look back\n",
        "        forecast_horizon=5,      # Number of steps to predict ahead\n",
        "        img_height=128,         # Resize images to this height\n",
        "        img_width=128,          # Resize images to this width\n",
        "        channels=1,             # Grayscale images\n",
        "        target_features=target_features,\n",
        "        include_power_feature=True  # Include laser power as input feature\n",
        "    )\n",
        "\n",
        "    # 7. Fit scalers on training data only\n",
        "    train_data = df_combined[df_combined['trial'].isin(train_trials)].copy() # Use copy to avoid SettingWithCopyWarning\n",
        "\n",
        "    # Check if training data is empty before fitting scalers\n",
        "    if len(train_data) == 0:\n",
        "        raise ValueError(\"Training data is empty after splitting trials. Cannot fit scalers.\")\n",
        "\n",
        "    # Drop rows with NaN in target features or laser power before fitting scalers\n",
        "    cols_to_check_for_nan = list(target_features) + ['laser_power']\n",
        "    train_data.dropna(subset=cols_to_check_for_nan, inplace=True)\n",
        "\n",
        "    if len(train_data) == 0:\n",
        "        raise ValueError(\"Training data is empty after dropping rows with missing target features or laser power. Cannot fit scalers.\")\n",
        "\n",
        "    forecaster.target_scaler.fit(train_data[target_features])\n",
        "    forecaster.power_scaler.fit(train_data[['laser_power']])\n",
        "    print(f\"\\nFitted scalers on {len(train_data)} training samples (after dropping NaNs)\")\n",
        "    print(f\"Power range in training data: {train_data['laser_power'].min()}W - {train_data['laser_power'].max()}W\")\n",
        "\n",
        "    # 8. Train the model\n",
        "    print(f\"\\n\" + \"=\"*50)\n",
        "    print(\"STARTING TRAINING WITH LASER POWER FEATURE\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"This process may take 30-60 minutes...\")\n",
        "    print(\"The model will learn to predict keyhole behavior based on both\")\n",
        "    print(\"image sequences AND laser power levels\")\n",
        "\n",
        "    try:\n",
        "        history = forecaster.train(\n",
        "            df_combined,                # Combined dataframe\n",
        "            epochs=50,                  # Number of training epochs\n",
        "            batch_size=8,              # Batch size (adjust based on GPU memory)\n",
        "            split_strategy='trial',     # Use trial-based splitting\n",
        "            test_trials=test_trials,    # Trials for testing\n",
        "            val_trials=val_trials      # Trials for validation\n",
        "        )\n",
        "\n",
        "        print(\"✓ Training completed successfully!\")\n",
        "\n",
        "        # 9. Evaluate the model\n",
        "        print(f\"\\n\" + \"=\"*50)\n",
        "        print(\"MODEL EVALUATION\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        if forecaster.X_test is not None and len(forecaster.X_test) > 0:\n",
        "            results, predictions = forecaster.evaluate_model()\n",
        "\n",
        "            print(\"Overall Model Performance:\")\n",
        "            print(\"-\" * 40)\n",
        "            for feature, metrics in results.items():\n",
        "                print(f\"{feature}:\")\n",
        "                print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
        "                print(f\"  MAE:  {metrics['mae']:.4f}\")\n",
        "                print(f\"  R²:   {metrics['r2']:.4f}\")\n",
        "                print()\n",
        "\n",
        "            # 10. Power-specific analysis\n",
        "            print(\"Analyzing performance by laser power...\")\n",
        "            analyze_power_vs_performance(forecaster, df_combined)\n",
        "\n",
        "            # 11. Plot results\n",
        "            print(\"Generating plots...\")\n",
        "            forecaster.plot_training_history()\n",
        "            forecaster.plot_predictions(n_samples=3)\n",
        "            plot_power_analysis(forecaster, df_combined)\n",
        "\n",
        "        else:\n",
        "            print(\"No test data available for evaluation\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Training failed: {e}\")\n",
        "        print(\"\\nCommon solutions:\")\n",
        "        print(\"1. Reduce batch_size (try 4 or 2) if you get memory errors\")\n",
        "        print(\"2. Reduce sequence_length if you have insufficient data\")\n",
        "        print(\"3. Check that image paths are correct and images load properly\")\n",
        "        print(\"4. Ensure each trial has enough samples for sequence generation\")\n",
        "        print(\"5. Verify target features exist and contain valid numeric data\")\n",
        "        print(\"6. Check that laser power values are extracted correctly from filenames\")\n",
        "        print(\"7. Ensure there are enough valid trials for train/val/test splits.\")\n",
        "\n",
        "\n",
        "        # Print debug information\n",
        "        print(f\"\\nDebug Info:\")\n",
        "        if 'df_combined' in locals():\n",
        "            print(f\"  Total samples in combined_df: {len(df_combined)}\")\n",
        "            print(f\"  Trials in combined_df: {len(df_combined['trial'].unique()) if 'trial' in df_combined.columns else 0}\")\n",
        "            if 'train_data' in locals():\n",
        "                print(f\"  Training samples (after NaN drop): {len(train_data)}\")\n",
        "            if 'forecaster' in locals() and hasattr(forecaster, 'sequence_length'):\n",
        "                 print(f\"  Required samples per trial for sequence generation: {forecaster.sequence_length + forecaster.forecast_horizon - 1}\")\n",
        "            if 'train_trials' in locals():\n",
        "                 print(f\"  Train/Val/Test trials (after filtering): {len(train_trials)}/{len(val_trials)}/{len(test_trials)}\")\n",
        "            if not df_combined.empty and 'laser_power' in df_combined.columns:\n",
        "                 print(f\"  Power range in combined_df: {df_combined['laser_power'].min()}W - {df_combined['laser_power'].max()}W\")\n",
        "        else:\n",
        "            print(\"  df_combined is not defined. Data loading failed early.\")\n",
        "\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"SETUP SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    if 'df_combined' in locals():\n",
        "        print(f\"Dataset: {len(df_combined)} samples across {len(df_combined['trial'].unique()) if 'trial' in df_combined.columns else 0} trials\")\n",
        "        if 'forecaster' in locals() and hasattr(forecaster, 'img_height'):\n",
        "            print(f\"Image size: {forecaster.img_height}x{forecaster.img_width}\")\n",
        "            print(f\"Sequence length: {forecaster.sequence_length}\")\n",
        "            print(f\"Forecast horizon: {forecaster.forecast_horizon}\")\n",
        "            print(f\"Target features: {target_features}\")\n",
        "            print(f\"Laser power feature: {'Enabled' if forecaster.include_power_feature else 'Disabled'}\")\n",
        "        if not df_combined.empty and 'laser_power' in df_combined.columns:\n",
        "             print(f\"Power range: {df_combined['laser_power'].min()}W - {df_combined['laser_power'].max()}W\")\n",
        "        if 'train_trials' in locals():\n",
        "             print(f\"Train/Val/Test trials: {len(train_trials)}/{len(val_trials)}/{len(test_trials)}\")\n",
        "    else:\n",
        "        print(\"Data loading failed.\")\n",
        "\n",
        "\n",
        "    if 'history' in locals():\n",
        "        print(f\"Training epochs completed: {len(history.history['loss'])}\")\n",
        "        print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
        "        if 'val_loss' in history.history:\n",
        "            print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"NEXT STEPS\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"1. The model now considers laser power in its predictions\")\n",
        "    print(\"2. You can predict on new sequences with different power levels\")\n",
        "    print(\"3. Save your model: forecaster.model.save('cnn_lstm_with_power.h5')\")\n",
        "    print(\"4. Experiment with different power levels for process optimization\")\n",
        "    print(\"5. Use power-specific analysis to identify optimal operating conditions\")\n",
        "\n",
        "    if 'forecaster' in locals() and forecaster.model is not None:\n",
        "         print(\"\\nModel ready for power-aware predictions!\")\n",
        "    else:\n",
        "         print(\"\\nModel training failed. Please check debug information above.\")\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# UTILITY FUNCTIONS FOR POWER-AWARE PREDICTIONS\n",
        "# ====================================================================\n",
        "\n",
        "def predict_with_power(forecaster, image_sequence, power_level):\n",
        "    \"\"\"\n",
        "    Make predictions for a given image sequence and power level\n",
        "\n",
        "    Parameters:\n",
        "    - forecaster: Trained CNN-LSTM forecaster\n",
        "    - image_sequence: Sequence of images (sequence_length, height, width, channels)\n",
        "    - power_level: Laser power in Watts\n",
        "\n",
        "    Returns:\n",
        "    - Predicted keyhole dimensions\n",
        "    \"\"\"\n",
        "    if not forecaster.include_power_feature:\n",
        "        print(\"Model was not trained with power feature\")\n",
        "        return None\n",
        "\n",
        "    if len(image_sequence.shape) == 4:\n",
        "        image_sequence = image_sequence.reshape(1, forecaster.sequence_length,\n",
        "                                              forecaster.img_height, forecaster.img_width, forecaster.channels)\n",
        "\n",
        "    # Normalize power\n",
        "    power_normalized = forecaster.power_scaler.transform([[power_level]])[0]\n",
        "    power_input = power_normalized.reshape(1, 1)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = forecaster.model.predict([image_sequence, power_input], verbose=0)\n",
        "\n",
        "    # Inverse transform to original scale\n",
        "    prediction_reshaped = prediction.reshape(-1, forecaster.n_features)\n",
        "    prediction_orig = forecaster.target_scaler.inverse_transform(prediction_reshaped)\n",
        "\n",
        "    return prediction_orig.reshape(forecaster.forecast_horizon, forecaster.n_features)\n",
        "\n",
        "\n",
        "def power_optimization_study(forecaster, sample_image_sequence, power_range=(200, 450, 50)):\n",
        "    \"\"\"\n",
        "    Study the effect of different power levels on predictions\n",
        "\n",
        "    Parameters:\n",
        "    - forecaster: Trained CNN-LSTM forecaster\n",
        "    - sample_image_sequence: Sample image sequence for analysis\n",
        "    - power_range: (min_power, max_power, step) in Watts\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary with power levels and corresponding predictions\n",
        "    \"\"\"\n",
        "    if not forecaster.include_power_feature:\n",
        "        print(\"Model was not trained with power feature\")\n",
        "        return None\n",
        "\n",
        "    powers = range(power_range[0], power_range[1] + 1, power_range[2])\n",
        "    results = {}\n",
        "\n",
        "    for power in powers:\n",
        "        prediction = predict_with_power(forecaster, sample_image_sequence, power)\n",
        "        results[power] = prediction\n",
        "\n",
        "        print(f\"Power {power}W predictions:\")\n",
        "        for i, feature in enumerate(forecaster.target_features):\n",
        "            avg_pred = np.mean(prediction[:, i])\n",
        "            print(f\"  {feature}: {avg_pred:.4f}\")\n",
        "        print()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def plot_power_optimization(power_results, forecaster):\n",
        "    \"\"\"\n",
        "    Plot the results of power optimization study\n",
        "    \"\"\"\n",
        "    if not power_results:\n",
        "        return\n",
        "\n",
        "    powers = sorted(power_results.keys())\n",
        "    n_features = len(forecaster.target_features)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, feature in enumerate(forecaster.target_features):\n",
        "        if i >= len(axes):\n",
        "            break\n",
        "\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Get average predictions for each power level\n",
        "        avg_predictions = []\n",
        "        for power in powers:\n",
        "            avg_pred = np.mean(power_results[power][:, i])\n",
        "            avg_predictions.append(avg_pred)\n",
        "\n",
        "        ax.plot(powers, avg_predictions, 'o-', linewidth=2, markersize=8)\n",
        "        ax.set_xlabel('Laser Power (W)')\n",
        "        ax.set_ylabel(f'Predicted {feature}')\n",
        "        ax.set_title(f'Effect of Laser Power on {feature}')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52240QCB4jvm",
        "outputId": "38c36b06-ad0d-4818-ed31-284f16da3cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CNN-LSTM KEYHOLE FORECASTER WITH LASER POWER\n",
            "============================================================\n",
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Image base path: /content/drive/MyDrive/label_keyhole_porosity/AM_image\n",
            "Label base path: /content/drive/MyDrive/label_keyhole_porosity/AM_label\n",
            "\n",
            "Loading data from folders...\n",
            "Found 12 CSV files: ['AM_label_007.csv', 'AM_label_009.csv', 'AM_label_019.csv', 'AM_label_005.csv', 'AM_label_016.csv', 'AM_label_008.csv', 'AM_label_015.csv', 'AM_label_011.csv', 'AM_label_018.csv', 'AM_label_006.csv', 'AM_label_010.csv', 'AM_label_017.csv']\n",
            "\n",
            "Loading AM_label_007.csv: (501, 7)\n",
            "Columns: ['abs_index', 'rel_index', 'v2_keyhole_depth/pixel', 'v2_keyhole_width/pixel', 'v2_keyhole_depth/um', 'v2_keyhole_width/um', 'porosity']\n",
            "Found 501 images in 007\n",
            "Sample image filename: 007_Al6061_P250W_t4ms_U18G16_spot0255_original.jpg\n",
            "Inferred image path template: /content/drive/MyDrive/label_keyhole_porosity/AM_image/007/007_Al6061_P250W_t4ms_U18G16_spot{x:04d}_original.jpg\n",
            "Generated image paths using column 'abs_index' and template.\n",
            "✓ Verified image paths exist\n",
            "Laser powers in this trial: [250]\n",
            "✓ Loaded 501 samples from 007\n",
            "\n",
            "Loading AM_label_009.csv: (501, 7)\n",
            "Columns: ['abs_index', 'rel_index', 'v2_keyhole_depth/pixel', 'v2_keyhole_width/pixel', 'v2_keyhole_depth/um', 'v2_keyhole_width/um', 'porosity']\n",
            "Found 501 images in 009\n",
            "Sample image filename: 009_Al6061_P450W_t4ms_U18G16_spot0255_original.jpg\n",
            "Inferred image path template: /content/drive/MyDrive/label_keyhole_porosity/AM_image/009/009_Al6061_P450W_t4ms_U18G16_spot{x:04d}_original.jpg\n",
            "Generated image paths using column 'abs_index' and template.\n",
            "✓ Verified image paths exist\n",
            "Laser powers in this trial: [450]\n",
            "✓ Loaded 501 samples from 009\n",
            "\n",
            "Loading AM_label_019.csv: (501, 7)\n",
            "Columns: ['abs_index', 'rel_index', 'v2_keyhole_depth/pixel', 'v2_keyhole_width/pixel', 'v2_keyhole_depth/um', 'v2_keyhole_width/um', 'porosity']\n",
            "Found 501 images in 019\n",
            "Sample image filename: 019_Al6061_P350W_t4ms_U18G16_spot0255_original.jpg\n",
            "Inferred image path template: /content/drive/MyDrive/label_keyhole_porosity/AM_image/019/019_Al6061_P350W_t4ms_U18G16_spot{x:04d}_original.jpg\n",
            "Generated image paths using column 'abs_index' and template.\n",
            "✓ Verified image paths exist\n",
            "Laser powers in this trial: [350]\n",
            "✓ Loaded 501 samples from 019\n",
            "\n",
            "Loading AM_label_005.csv: (501, 7)\n",
            "Columns: ['abs_index', 'rel_index', 'v2_keyhole_depth/pixel', 'v2_keyhole_width/pixel', 'v2_keyhole_depth/um', 'v2_keyhole_width/um', 'porosity']\n",
            "Found 501 images in 005\n",
            "Sample image filename: 005_Al6061_P400W_t4ms_U18G16_spot0255_original.jpg\n",
            "Inferred image path template: /content/drive/MyDrive/label_keyhole_porosity/AM_image/005/005_Al6061_P400W_t4ms_U18G16_spot{x:04d}_original.jpg\n",
            "Generated image paths using column 'abs_index' and template.\n",
            "✓ Verified image paths exist\n",
            "Laser powers in this trial: [400]\n",
            "✓ Loaded 501 samples from 005\n",
            "\n",
            "Loading AM_label_016.csv: (501, 7)\n",
            "Columns: ['abs_index', 'rel_index', 'v2_keyhole_depth/pixel', 'v2_keyhole_width/pixel', 'v2_keyhole_depth/um', 'v2_keyhole_width/um', 'porosity']\n",
            "Found 501 images in 016\n",
            "Sample image filename: 016_Al6061_P250W_t4ms_U18G16_spot0255_original.jpg\n",
            "Inferred image path template: /content/drive/MyDrive/label_keyhole_porosity/AM_image/016/016_Al6061_P250W_t4ms_U18G16_spot{x:04d}_original.jpg\n",
            "Generated image paths using column 'abs_index' and template.\n",
            "✓ Verified image paths exist\n",
            "Laser powers in this trial: [250]\n",
            "✓ Loaded 501 samples from 016\n",
            "\n",
            "Loading AM_label_008.csv: (501, 7)\n",
            "Columns: ['abs_index', 'rel_index', 'v2_keyhole_depth/pixel', 'v2_keyhole_width/pixel', 'v2_keyhole_depth/um', 'v2_keyhole_width/um', 'porosity']\n",
            "Found 501 images in 008\n",
            "Sample image filename: 008_Al6061_P200W_t4ms_U18G16_spot0255_original.jpg\n",
            "Inferred image path template: /content/drive/MyDrive/label_keyhole_porosity/AM_image/008/008_Al6061_P200W_t4ms_U18G16_spot{x:04d}_original.jpg\n",
            "Generated image paths using column 'abs_index' and template.\n",
            "✓ Verified image paths exist\n",
            "Laser powers in this trial: [200]\n",
            "✓ Loaded 501 samples from 008\n",
            "\n",
            "Loading AM_label_015.csv: (501, 7)\n",
            "Columns: ['abs_index', 'rel_index', 'v2_keyhole_depth/pixel', 'v2_keyhole_width/pixel', 'v2_keyhole_depth/um', 'v2_keyhole_width/um', 'porosity']\n",
            "Found 501 images in 015\n",
            "Sample image filename: 015_Al6061_P300W_t4ms_U18G16_spot0255_original.jpg\n",
            "Inferred image path template: /content/drive/MyDrive/label_keyhole_porosity/AM_image/015/015_Al6061_P300W_t4ms_U18G16_spot{x:04d}_original.jpg\n",
            "Generated image paths using column 'abs_index' and template.\n",
            "✓ Verified image paths exist\n",
            "Laser powers in this trial: [300]\n",
            "✓ Loaded 501 samples from 015\n",
            "\n",
            "Loading AM_label_011.csv: (501, 11)\n",
            "Columns: ['abs_index', 'rel_index', 'v2_keyhole_depth/pixel', 'v2_keyhole_width/pixel', 'v2_keyhole_depth/um', 'v2_keyhole_width/um', 'porosity', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10']\n",
            "Found 501 images in 011\n",
            "Sample image filename: 011_Al6061_P350W_t4ms_U18G16_spot0255_original.jpg\n",
            "Inferred image path template: /content/drive/MyDrive/label_keyhole_porosity/AM_image/011/011_Al6061_P350W_t4ms_U18G16_spot{x:04d}_original.jpg\n",
            "Generated image paths using column 'abs_index' and template.\n",
            "✓ Verified image paths exist\n",
            "Laser powers in this trial: [350]\n",
            "✓ Loaded 501 samples from 011\n",
            "\n",
            "Loading AM_label_018.csv: (501, 7)\n",
            "Columns: ['abs_index', 'rel_index', 'v2_keyhole_depth/pixel', 'v2_keyhole_width/pixel', 'v2_keyhole_depth/um', 'v2_keyhole_width/um', 'porosity']\n",
            "Found 501 images in 018\n",
            "Sample image filename: 018_Al6061_P450W_t4ms_U18G16_spot0255_original.jpg\n",
            "Inferred image path template: /content/drive/MyDrive/label_keyhole_porosity/AM_image/018/018_Al6061_P450W_t4ms_U18G16_spot{x:04d}_original.jpg\n",
            "Generated image paths using column 'abs_index' and template.\n",
            "✓ Verified image paths exist\n",
            "Laser powers in this trial: [450]\n",
            "✓ Loaded 501 samples from 018\n",
            "\n",
            "Loading AM_label_006.csv: (501, 7)\n",
            "Columns: ['abs_index', 'rel_index', 'v2_keyhole_depth/pixel', 'v2_keyhole_width/pixel', 'v2_keyhole_depth/um', 'v2_keyhole_width/um', 'porosity']\n",
            "Found 501 images in 006\n",
            "Sample image filename: 006_Al6061_P300W_t4ms_U18G16_spot0255_original.jpg\n",
            "Inferred image path template: /content/drive/MyDrive/label_keyhole_porosity/AM_image/006/006_Al6061_P300W_t4ms_U18G16_spot{x:04d}_original.jpg\n",
            "Generated image paths using column 'abs_index' and template.\n",
            "✓ Verified image paths exist\n",
            "Laser powers in this trial: [300]\n",
            "✓ Loaded 501 samples from 006\n",
            "\n",
            "Loading AM_label_010.csv: (501, 7)\n",
            "Columns: ['abs_index', 'rel_index', 'v2_keyhole_depth/pixel', 'v2_keyhole_width/pixel', 'v2_keyhole_depth/um', 'v2_keyhole_width/um', 'porosity']\n",
            "Found 501 images in 010\n",
            "Sample image filename: 010_Al6061_P400W_t4ms_U18G16_spot0255_original.jpg\n",
            "Inferred image path template: /content/drive/MyDrive/label_keyhole_porosity/AM_image/010/010_Al6061_P400W_t4ms_U18G16_spot{x:04d}_original.jpg\n",
            "Generated image paths using column 'abs_index' and template.\n",
            "✓ Verified image paths exist\n",
            "Laser powers in this trial: [400]\n",
            "✓ Loaded 501 samples from 010\n",
            "\n",
            "Loading AM_label_017.csv: (501, 7)\n",
            "Columns: ['abs_index', 'rel_index', 'v2_keyhole_depth/pixel', 'v2_keyhole_width/pixel', 'v2_keyhole_depth/um', 'v2_keyhole_width/um', 'porosity']\n",
            "Found 501 images in 017\n",
            "Sample image filename: 017_Al6061_P200W_t4ms_U18G16_spot0255_original.jpg\n",
            "Inferred image path template: /content/drive/MyDrive/label_keyhole_porosity/AM_image/017/017_Al6061_P200W_t4ms_U18G16_spot{x:04d}_original.jpg\n",
            "Generated image paths using column 'abs_index' and template.\n",
            "✓ Verified image paths exist\n",
            "Laser powers in this trial: [200]\n",
            "✓ Loaded 501 samples from 017\n",
            "\n",
            "==================================================\n",
            "COMBINED DATASET\n",
            "==================================================\n",
            "Total samples: 6012\n",
            "Trials: ['005', '006', '007', '008', '009', '010', '011', '015', '016', '017', '018', '019']\n",
            "Laser power range: 200W - 450W\n",
            "Unique laser powers: [np.int64(200), np.int64(250), np.int64(300), np.int64(350), np.int64(400), np.int64(450)]\n",
            "Sample image path: /content/drive/MyDrive/label_keyhole_porosity/AM_image/007/007_Al6061_P250W_t4ms_U18G16_spot0255_original.jpg\n",
            "Image exists: True\n",
            "\n",
            "Available columns: ['abs_index', 'rel_index', 'v2_keyhole_depth/pixel', 'v2_keyhole_width/pixel', 'v2_keyhole_depth/um', 'v2_keyhole_width/um', 'porosity', 'image_path', 'laser_power', 'spot_index', 'trial', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10']\n",
            "Using target features: ['v2_keyhole_depth/pixel', 'v2_keyhole_width/pixel', 'v2_keyhole_depth/um', 'v2_keyhole_width/um', 'porosity']\n",
            "\n",
            "Target feature statistics:\n",
            "       v2_keyhole_depth/pixel  v2_keyhole_width/pixel  v2_keyhole_depth/um  \\\n",
            "count             6012.000000             6012.000000          6012.000000   \n",
            "mean                75.750904               32.758100           151.501808   \n",
            "std                 85.305427               49.936815           170.610854   \n",
            "min                  0.000000                0.000000             0.000000   \n",
            "25%                  0.000000                0.000000             0.000000   \n",
            "50%                 22.000000               21.000000            44.000000   \n",
            "75%                149.131750               40.000000           298.263500   \n",
            "max                335.181000              225.080000           670.362000   \n",
            "\n",
            "       v2_keyhole_width/um     porosity  \n",
            "count          6012.000000  6011.000000  \n",
            "mean             65.516200     0.279820  \n",
            "std              99.873629     0.448948  \n",
            "min               0.000000     0.000000  \n",
            "25%               0.000000     0.000000  \n",
            "50%              42.000000     0.000000  \n",
            "75%              80.000000     1.000000  \n",
            "max             450.160000     1.000000  \n",
            "\n",
            "Trial power distribution (from 12 valid trials):\n",
            "  005: 400W\n",
            "  006: 300W\n",
            "  007: 250W\n",
            "  008: 200W\n",
            "  009: 450W\n",
            "  010: 400W\n",
            "  011: 350W\n",
            "  015: 300W\n",
            "  016: 250W\n",
            "  017: 200W\n",
            "  018: 450W\n",
            "  019: 350W\n",
            "\n",
            "Stratified trial splits (seed=42):\n",
            "  Train trials (8): ['017', '008', '007', '016', '015', '006', '019', '011']\n",
            "  Val trials (1): ['010']\n",
            "  Test trials (3): ['005', '018', '009']\n",
            "  Train: 4008 samples, Powers: [np.int64(200), np.int64(250), np.int64(300), np.int64(350)]W\n",
            "  Val: 501 samples, Powers: [np.int64(400)]W\n",
            "  Test: 1503 samples, Powers: [np.int64(400), np.int64(450)]W\n",
            "\n",
            "Fitted scalers on 4008 training samples (after dropping NaNs)\n",
            "Power range in training data: 200W - 350W\n",
            "\n",
            "==================================================\n",
            "STARTING TRAINING WITH LASER POWER FEATURE\n",
            "==================================================\n",
            "This process may take 30-60 minutes...\n",
            "The model will learn to predict keyhole behavior based on both\n",
            "image sequences AND laser power levels\n",
            "Processing trial 007: 501 samples\n",
            "  Generated 477 sequences\n",
            "Processing trial 009: 501 samples\n",
            "  Generated 477 sequences\n",
            "Processing trial 019: 501 samples\n",
            "  Generated 477 sequences\n",
            "Processing trial 005: 501 samples\n",
            "  Generated 477 sequences\n",
            "Processing trial 016: 501 samples\n",
            "  Generated 477 sequences\n",
            "Processing trial 008: 501 samples\n",
            "  Generated 477 sequences\n",
            "Processing trial 015: 501 samples\n",
            "  Generated 477 sequences\n",
            "Processing trial 011: 501 samples\n",
            "  Generated 477 sequences\n",
            "Processing trial 018: 501 samples\n",
            "  Generated 477 sequences\n",
            "Processing trial 006: 501 samples\n",
            "  Generated 477 sequences\n",
            "Processing trial 010: 501 samples\n",
            "  Generated 477 sequences\n",
            "Processing trial 017: 501 samples\n",
            "  Generated 477 sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# UTILITY FUNCTIONS FOR POST-TRAINING ANALYSIS\n",
        "# ====================================================================\n",
        "\n",
        "def analyze_trial_predictions(forecaster, df_combined, trial_name, n_sequences=5):\n",
        "    \"\"\"\n",
        "    Analyze predictions for a specific trial\n",
        "\n",
        "    Parameters:\n",
        "    - forecaster: Trained CNN-LSTM forecaster\n",
        "    - df_combined: Combined dataframe\n",
        "    - trial_name: Name of trial to analyze\n",
        "    - n_sequences: Number of sequences to analyze\n",
        "    \"\"\"\n",
        "    if forecaster.model is None:\n",
        "        print(\"Model not trained yet!\")\n",
        "        return\n",
        "\n",
        "    trial_data = df_combined[df_combined['trial'] == trial_name].sort_values('abs_index')\n",
        "    print(f\"Analyzing trial: {trial_name}\")\n",
        "    print(f\"Trial data shape: {trial_data.shape}\")\n",
        "\n",
        "    # Load trial sequences\n",
        "    trial_X, trial_y = forecaster.load_trial_sequences(trial_name, trial_data)\n",
        "\n",
        "    if len(trial_X) == 0:\n",
        "        print(\"No sequences generated for this trial\")\n",
        "        return\n",
        "\n",
        "    # Make predictions\n",
        "    n_sequences = min(n_sequences, len(trial_X))\n",
        "    predictions = forecaster.model.predict(trial_X[:n_sequences], verbose=0)\n",
        "\n",
        "    # Inverse transform\n",
        "    y_true_reshaped = trial_y[:n_sequences].reshape(-1, forecaster.n_features)\n",
        "    y_pred_reshaped = predictions.reshape(-1, forecaster.n_features)\n",
        "\n",
        "    y_true_orig = forecaster.target_scaler.inverse_transform(y_true_reshaped)\n",
        "    y_pred_orig = forecaster.target_scaler.inverse_transform(y_pred_reshaped)\n",
        "\n",
        "    # Calculate metrics\n",
        "    for i, feature in enumerate(forecaster.target_features):\n",
        "        y_true_feat = y_true_orig[:, i]\n",
        "        y_pred_feat = y_pred_orig[:, i]\n",
        "\n",
        "        mse = mean_squared_error(y_true_feat, y_pred_feat)\n",
        "        mae = mean_absolute_error(y_true_feat, y_pred_feat)\n",
        "        r2 = r2_score(y_true_feat, y_pred_feat)\n",
        "\n",
        "        print(f\"{feature}: RMSE={np.sqrt(mse):.4f}, MAE={mae:.4f}, R²={r2:.4f}\")\n",
        "\n",
        "    return predictions, trial_X, trial_y\n",
        "\n",
        "\n",
        "def save_model_and_scaler(forecaster, base_path=\"/content/drive/MyDrive/saved_models\"):\n",
        "    \"\"\"\n",
        "    Save the trained model and scaler\n",
        "\n",
        "    Parameters:\n",
        "    - forecaster: Trained CNN-LSTM forecaster\n",
        "    - base_path: Path to save model files\n",
        "    \"\"\"\n",
        "    import pickle\n",
        "    import os\n",
        "\n",
        "    if forecaster.model is None:\n",
        "        print(\"No model to save!\")\n",
        "        return\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "    # Save model\n",
        "    model_path = os.path.join(base_path, \"cnn_lstm_keyhole_model.h5\")\n",
        "    forecaster.model.save(model_path)\n",
        "    print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "    # Save scaler\n",
        "    scaler_path = os.path.join(base_path, \"target_scaler.pkl\")\n",
        "    with open(scaler_path, 'wb') as f:\n",
        "        pickle.dump(forecaster.target_scaler, f)\n",
        "    print(f\"Scaler saved to: {scaler_path}\")\n",
        "\n",
        "    # Save configuration\n",
        "    config = {\n",
        "        'sequence_length': forecaster.sequence_length,\n",
        "        'forecast_horizon': forecaster.forecast_horizon,\n",
        "        'img_height': forecaster.img_height,\n",
        "        'img_width': forecaster.img_width,\n",
        "        'channels': forecaster.channels,\n",
        "        'target_features': forecaster.target_features\n",
        "    }\n",
        "\n",
        "    config_path = os.path.join(base_path, \"model_config.pkl\")\n",
        "    with open(config_path, 'wb') as f:\n",
        "        pickle.dump(config, f)\n",
        "    print(f\"Configuration saved to: {config_path}\")\n",
        "\n",
        "\n",
        "def load_model_and_scaler(base_path=\"/content/drive/MyDrive/saved_models\"):\n",
        "    \"\"\"\n",
        "    Load a saved model and create forecaster\n",
        "\n",
        "    Parameters:\n",
        "    - base_path: Path where model files are saved\n",
        "\n",
        "    Returns:\n",
        "    - Loaded forecaster object\n",
        "    \"\"\"\n",
        "    import pickle\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # Load configuration\n",
        "    config_path = os.path.join(base_path, \"model_config.pkl\")\n",
        "    with open(config_path, 'rb') as f:\n",
        "        config = pickle.load(f)\n",
        "\n",
        "    # Create forecaster\n",
        "    forecaster = CNNLSTMKeyholeForecaster(**config)\n",
        "\n",
        "    # Load model\n",
        "    model_path = os.path.join(base_path, \"cnn_lstm_keyhole_model.h5\")\n",
        "    forecaster.model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "    # Load scaler\n",
        "    scaler_path = os.path.join(base_path, \"target_scaler.pkl\")\n",
        "    with open(scaler_path, 'rb') as f:\n",
        "        forecaster.target_scaler = pickle.load(f)\n",
        "\n",
        "    print(f\"Model loaded from: {model_path}\")\n",
        "    print(f\"Scaler loaded from: {scaler_path}\")\n",
        "\n",
        "    return forecaster\n",
        "\n",
        "\n",
        "def predict_on_new_trial(forecaster, image_folder_path, start_sequence_length=None):\n",
        "    \"\"\"\n",
        "    Make predictions on a new trial (folder of images)\n",
        "\n",
        "    Parameters:\n",
        "    - forecaster: Trained CNN-LSTM forecaster\n",
        "    - image_folder_path: Path to folder containing images\n",
        "    - start_sequence_length: Length of initial sequence (uses forecaster.sequence_length if None)\n",
        "\n",
        "    Returns:\n",
        "    - Array of predictions\n",
        "    \"\"\"\n",
        "    if forecaster.model is None:\n",
        "        print(\"Model not trained yet!\")\n",
        "        return None\n",
        "\n",
        "    if start_sequence_length is None:\n",
        "        start_sequence_length = forecaster.sequence_length\n",
        "\n",
        "    # Get image files\n",
        "    image_files = sorted([f for f in os.listdir(image_folder_path)\n",
        "                         if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff'))])\n",
        "\n",
        "    if len(image_files) < start_sequence_length:\n",
        "        print(f\"Need at least {start_sequence_length} images, found {len(image_files)}\")\n",
        "        return None\n",
        "\n",
        "    # Load and preprocess images\n",
        "    image_paths = [os.path.join(image_folder_path, f) for f in image_files]\n",
        "    images = forecaster.load_and_preprocess_images(image_paths[:start_sequence_length])\n",
        "\n",
        "    # Reshape for prediction\n",
        "    input_sequence = images.reshape(1, start_sequence_length,\n",
        "                                  forecaster.img_height, forecaster.img_width, forecaster.channels)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = forecaster.model.predict(input_sequence, verbose=0)\n",
        "\n",
        "    # Inverse transform to original scale\n",
        "    prediction_reshaped = prediction.reshape(-1, forecaster.n_features)\n",
        "    prediction_orig = forecaster.target_scaler.inverse_transform(prediction_reshaped)\n",
        "\n",
        "    print(f\"Prediction for {len(image_files)} images:\")\n",
        "    for i, feature in enumerate(forecaster.target_features):\n",
        "        print(f\"{feature}: {prediction_orig[:, i]}\")\n",
        "\n",
        "    return prediction_orig\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# EXAMPLE USAGE AFTER TRAINING\n",
        "# ====================================================================\n",
        "\n",
        "def example_post_training_analysis():\n",
        "    \"\"\"\n",
        "    Example of how to use the forecaster after training\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXAMPLE POST-TRAINING ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # This assumes you have a trained forecaster and data loaded\n",
        "    # Uncomment and modify as needed:\n",
        "\n",
        "    \"\"\"\n",
        "    # 1. Analyze specific trial\n",
        "    analyze_trial_predictions(forecaster, df_combined, 'trial_001', n_sequences=3)\n",
        "\n",
        "    # 2. Save model for future use\n",
        "    save_model_and_scaler(forecaster, \"/content/drive/MyDrive/my_models\")\n",
        "\n",
        "    # 3. Load saved model (in a new session)\n",
        "    # loaded_forecaster = load_model_and_scaler(\"/content/drive/MyDrive/my_models\")\n",
        "\n",
        "    # 4. Make predictions on new trial\n",
        "    # new_predictions = predict_on_new_trial(\n",
        "    #     forecaster,\n",
        "    #     \"/content/drive/MyDrive/new_trial_images\"\n",
        "    # )\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"See function definitions above for detailed usage examples\")\n",
        "    print(\"Modify the paths and parameters according to your needs\")"
      ],
      "metadata": {
        "id": "SR5aIHTg812N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}